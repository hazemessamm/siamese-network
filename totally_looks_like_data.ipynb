{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network \n",
    "\n",
    "[Siamese Network](https://en.wikipedia.org/wiki/Siamese_neural_network) is used to solve many problems like detecting question duplicates, face recognition by comparing the similarity of the inputs by comparing their feature vectors.\n",
    "\n",
    "First we need to have a dataset that contains 3 Images, 2 are similar and 1 is different, they are called Anchor image, Positive Image and Negative image respectively, we need to tell the network that the anchor image and the positive image are similar, we also need to tell it that the anchor image and the negative image are NOT similar, we can do that by the Triplet Loss Function.\n",
    "\n",
    "Triplet Loss function:\n",
    "\n",
    "L(Anchor, Positive, Negative) = max((distance(f(Anchor), f(Positive)) - distance(f(Anchor), f(Negative)))**2, 0.0)\n",
    "\n",
    "Note that the weights are shared which mean that we are only using one model for prediction and training\n",
    "\n",
    "You can find the dataset here: https://drive.google.com/drive/folders/1qQJHA5m-vLMAkBfWEWgGW9n61gC_orHl\n",
    "\n",
    "Also more info found here: https://sites.google.com/view/totally-looks-like-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get the paths of the datasets in siamese networks we usually have two folders each folder has images and every image has a corresponding similar picture in the other folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_images_path = os.path.join('C:/Users/Hazem/', 'left')\n",
    "right_images_path = os.path.join('C:/Users/Hazem/', 'right')\n",
    "img_shape = (245, 200, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SiameseDatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class inherits from Sequence which is used to generate images for training, the reason of using a generator is that there are datasets which contain a lot of high resolution images and we cannot load all of them in our memory so we just generate batches of them while training we inherit it so we can use it in training\n",
    "\n",
    "1) We override the __len__ method by returning our number of batches so keras can know how many batches available.\n",
    "2) We override __getitem__ method so we can access any index of an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative images are just random images we sample from our dataset. every example should contain 3 images (Anchor, Positive and Negative). The negative image should NOT be the same as the Anchor or the Positive images, We use a set() that stores the names of the anchor and positive images so when we sample the negative images we avoid getting any image that exist in the set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch shuffle:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to shuffle the batch so we can have random examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a list of paths for Anchor images, positive images, negative images we pass these lists to the preprocess_img()\n",
    "because we need to load the image given the path we have and we need to convert it into tensor by using img_to_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class SiameseDatasetGenerator(Sequence):\n",
    "        def __init__(\n",
    "            self,\n",
    "            anchor_images_path,\n",
    "            positive_images_path,\n",
    "            image_shape,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "        ):\n",
    "            self.anchor_images_path = (\n",
    "                anchor_images_path  # store the path of the anchor images\n",
    "            )\n",
    "            self.positive_images_path = (\n",
    "                positive_images_path  # store the path of the positive images\n",
    "            )\n",
    "            self.image_shape = image_shape  ##store image shape\n",
    "            # list the contents (images) of the specified directory\n",
    "            self.anchor_images = os.listdir(positive_images_path)\n",
    "            self.positive_images = os.listdir(positive_images_path)\n",
    "            assert batch_size > 0, \"Batch size should be greater than zero\"\n",
    "            self.batch_size = batch_size\n",
    "            self.num_examples = len(self.anchor_images)\n",
    "            self.num_batches = self.num_examples // batch_size\n",
    "            self.shuffle = shuffle\n",
    "            self.anchor_images = np.array(\n",
    "                [\n",
    "                    os.path.join(self.anchor_images_path + \"/\", img)\n",
    "                    for img in self.anchor_images\n",
    "                ]\n",
    "            )\n",
    "            self.positive_images = np.array(\n",
    "                [\n",
    "                    os.path.join(self.positive_images_path + \"/\", img)\n",
    "                    for img in self.positive_images\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "        we use __len__ method that is called\n",
    "        to get the length of the batches\n",
    "        it is called when we call len()\n",
    "        \"\"\"\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.num_batches\n",
    "\n",
    "        \"\"\"\n",
    "        this method allows us to get batches when we\n",
    "        access the instance the same way we access a list\n",
    "        e.g. dataset[0] will call __getitem__(index=0)\n",
    "        \"\"\"\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            # here we get batches of data by using slicing\n",
    "            anchor_imgs = self.anchor_images[\n",
    "                index * self.batch_size : (index + 1) * self.batch_size\n",
    "            ]\n",
    "            positive_imgs = self.positive_images[\n",
    "                index * self.batch_size : (index + 1) * self.batch_size\n",
    "            ]\n",
    "            # store the loaded images to avoid reloading them in negative images\n",
    "            # we store them in a set for faster access\n",
    "            loaded_examples = set(anchor_imgs)\n",
    "\n",
    "            # get negative_imgs by randomly choosing it from anchor or positive directory\n",
    "            negative_imgs = [\n",
    "                img\n",
    "                for img in random.choice([self.anchor_images, self.positive_images])\n",
    "                if img not in loaded_examples\n",
    "            ]\n",
    "            negative_imgs = np.array(random.choices(negative_imgs, k=self.batch_size))\n",
    "\n",
    "            if self.shuffle:\n",
    "                # create a list of random numbers to use it when we shuffle the batches\n",
    "                random_shuffle = random.choices(\n",
    "                    [*range(self.batch_size)], k=self.batch_size\n",
    "                )\n",
    "                anchor_imgs = anchor_imgs[random_shuffle]\n",
    "                positive_imgs = positive_imgs[random_shuffle]\n",
    "                negative_imgs = negative_imgs[random_shuffle]\n",
    "\n",
    "            anchor_imgs = self.preprocess_img(anchor_imgs)\n",
    "            positive_imgs = self.preprocess_img(positive_imgs)\n",
    "            negative_imgs = self.preprocess_img(negative_imgs)\n",
    "\n",
    "            # here if the batch size equal one we just convert the images into numpy\n",
    "            # and expand the dimension of this batch by adding 1 in the first axis\n",
    "            if self.batch_size == 1:\n",
    "                return tf.expand_dims(\n",
    "                    np.array([anchor_imgs, positive_imgs, negative_imgs]), axis=0\n",
    "                )\n",
    "            # Add the batch_size dimension in the first axis by using permute()\n",
    "            return backend.permute_dimensions(\n",
    "                np.array([anchor_imgs, positive_imgs, negative_imgs]), (1, 0, 2, 3, 4)\n",
    "            )\n",
    "\n",
    "        def preprocess_img(self, imgs):\n",
    "            # here we first load the images by using load_img()\n",
    "            # then we convert them into tensors by using img_to_array()\n",
    "            output = [image.img_to_array(image.load_img(img_path)) for img_path in imgs]\n",
    "            if len(output) == 1:\n",
    "                return output[0]\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SiameseDatasetGenerator(left_images_path, right_images_path, img_shape, 4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function just visalize each random 3 images (anchor, positive, negative)\n",
    "def visualize():\n",
    "    example = dataset[random.randint(0, dataset.batch_size)]\n",
    "    img1, img2, img3 = preprocessing.image.array_to_img(example[:, 0][0]), preprocessing.image.array_to_img(example[:, 1][0]), preprocessing.image.array_to_img(example[:, 2][0]) \n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    ax1.imshow(img1)\n",
    "    ax2.imshow(img2)\n",
    "    ax3.imshow(img3)\n",
    "    plt.savefig('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = tf.keras.utils.get_file(\n",
    "    \"example1.jpg\", \"https://imgur.com/t9OzTOm.png\"\n",
    ")\n",
    "\n",
    "display(Image(img_path)) #output example of visualize() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use ResNet50 architecture, we use \"imagenet\" weights, also we pass the image shape\n",
    "Note that include_top means that we do NOT want the top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cnn = applications.ResNet50(weights='imagenet', input_shape=img_shape, include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fine tune the ResNet50 we freeze all layers that exist before \"conv5_block1_out\" layer, starting from \"conv5_block2_2_relu\" layer we unfreeze all the layers so we can just train these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "    if layer.name == 'conv5_block1_out':\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding top layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we customize the model by adding Dense layers and Batch Normalization layers. we start with the image input then we pass the input to the base_cnn then we flatten it. Finally we pass each layer as an input to the next layer the output layer is just a dense layer which will act as an embedding for our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = layers.Flatten()(base_cnn.output)\n",
    "dense1 = layers.Dense(512, activation='relu')(flatten)\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation='relu')(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(base_cnn.input, output, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is just used for training we pass to it three input batches (anchor images, positive images, negative images) and the output will be the output of the model we defined above, it will be 1 output for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_input = layers.Input(shape=img_shape)\n",
    "positive_input = layers.Input(shape=img_shape)\n",
    "negative_input = layers.Input(shape=img_shape)\n",
    "\n",
    "anchor_output = embedding(anchor_input)\n",
    "positive_output = embedding(positive_input)\n",
    "negative_output = embedding(negative_input)\n",
    "\n",
    "training_model = Model([anchor_input, positive_input, negative_input], \n",
    "                       {\"anchor_embedding\": anchor_output, \n",
    "                        \"positive_embedding\": positive_output, \"negative_embedding\":negative_output})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This layer just computes how similar to feature vectors are by computing it using the Cosine Similarity\n",
    "We override the call method and implement our own call method.\n",
    "\n",
    "Check out https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity\n",
    "\n",
    "We return the negative of the loss because we just need to know how similar they are we do NOT need to know the loss\n",
    "\n",
    "losses.reduction.NONE means that if we want the predictions as it is,\n",
    "we don't want to reduce the predictions by sum or by mean,\n",
    "this is useful of we want to make an inference on a batch of inputs\n",
    "\n",
    "Check out https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineDistance(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CosineDistance, self).__init__()\n",
    "        \n",
    "    def call(self, img1, img2):\n",
    "        return -losses.CosineSimilarity(reduction=losses.Reduction.NONE)(img1, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we customize our training process and our model.\n",
    "\n",
    "We override the train_step() method and apply our own loss and our own training process\n",
    "\n",
    "We also use Triplet loss function as we specified above.\n",
    "\n",
    "Loss function explaination:\n",
    "\n",
    "we calculate the distance between the anchor embedding and the positive embedding the axis = -1 because we want the distance over the features of every example. We also add alpha which act as extra margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(Model):\n",
    "    def __init__(self, model, alpha=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.embedding = model #we pass the model to the class\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        pass\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        #here we create a tape to record our operations so we can get the gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            embeddings = training_model((data[:, 0], data[:, 1], data[:, 2]))\n",
    "            \n",
    "            #Euclidean Distance between anchor and positive\n",
    "            #axis=-1 so we can get distances over examples\n",
    "            anchor_positive_dist = tf.reduce_sum(\n",
    "                tf.square(embeddings['anchor_embedding'] - embeddings['positive_embedding']), -1)\n",
    "            \n",
    "            #Euclidean Distance between anchor and negative\n",
    "            anchor_negative_dist = tf.reduce_sum(\n",
    "                tf.square(embeddings['anchor_embedding'] - embeddings['negative_embedding']), -1)\n",
    "            \n",
    "            #getting the loss by subtracting the distances\n",
    "            loss = anchor_positive_dist - anchor_negative_dist\n",
    "            #getting the max because we don't want negative loss\n",
    "            loss = tf.reduce_sum(tf.maximum(loss+self.alpha, 0.0))\n",
    "        #getting the gradients [loss with respect to trainable weights]\n",
    "        grads = tape.gradient(loss, training_model.trainable_weights)\n",
    "        #applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(zip(grads, training_model.trainable_weights))\n",
    "        return {\"Loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = SiameseModel(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.compile(optimizer=optimizers.Adam(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.fit(dataset, epochs=2) #2 epochs because no enough computation power for more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we just load from the dataset an example\n",
    "#we should NOT test the performace of the model \n",
    "#using training data but here we are just see how did it learn\n",
    "example_prediction = dataset[9]\n",
    "anchor_example = image.array_to_img(example_prediction[:, 0][0])\n",
    "positive_example = image.array_to_img(example_prediction[:, 1][0])\n",
    "negative_example = image.array_to_img(example_prediction[:, 2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we just plotting the example that we loaded\n",
    "#f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "#ax1.imshow(anchor_example)\n",
    "#ax2.imshow(positive_example)\n",
    "#ax3.imshow(negative_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images of the code above\n",
    "img_path = tf.keras.utils.get_file(\n",
    "    \"example2.jpg\", \"https://imgur.com/0mhT8w6.png\"\n",
    ")\n",
    "\n",
    "display(Image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = tf.keras.utils.get_file(\n",
    "    \"example2.jpg\", \"https://imgur.com/t9OzTOm.png\"\n",
    ")\n",
    "\n",
    "display(Image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add an extra dimension (batch_size dimension) in the first axis by using expand dims.\n",
    "anchor_tensor = np.expand_dims(example_prediction[:, 0][0], axis=0)\n",
    "positive_tensor = np.expand_dims(example_prediction[:, 1][0], axis=0)\n",
    "negative_tensor = np.expand_dims(example_prediction[:, 2][0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embedding, positive_embedding = embedding(anchor_tensor), embedding(positive_tensor)\n",
    "positive_similarity = CosineDistance()(anchor_embedding, positive_embedding)\n",
    "print(\"Similarity:\", positive_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_embedding, negative_embedding = embedding(anchor_tensor), embedding(negative_tensor)\n",
    "negative_similarity = CosineDistance()(anchor_embedding, negative_embedding)\n",
    "print(\"Similarity:\", negative_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
